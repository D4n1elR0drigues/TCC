{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D4n1elR0drigues/TCC/blob/main/%5BMain_Code%5D_Trading_Robot_4_0_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTAR NA PRIMEIRA EXECUÇÃO!"
      ],
      "metadata": {
        "id": "ejU_s_D7j6b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader as data_reader\n",
        "\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "from collections import deque\n",
        "\n",
        "import pandas\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yfin\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "QTd21an3F2a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our Deep Q-Learning Trader\n",
        "\n",
        "class AI_Trader():\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # CONSTRUTOR\n",
        "\n",
        "  def __init__(self, state_size, action_space=3, model_name=\"AITrader\"):\n",
        "\n",
        "    self.state_size = state_size # Tamanho da entrada da rede neural\n",
        "    self.action_space = action_space # Espaço de ação será 3, Comprar, Vender, Sem Ação (Tamanho da saída da rede neural)\n",
        "    self.memory = deque(maxlen=2000) # Memória com 2000 posições. A função Deque permite adicionar elementos ao final, enquanto remove elementos do início.\n",
        "    self.inventory = [] # Terá as comprar que já fizemos\n",
        "    self.model_name = model_name # Nome do modelo para o Keras\n",
        "\n",
        "    self.gamma = 0.95 # Parâmetro que ajudará a maximizar a recompensa\n",
        "    self.epsilon = 1.0 # Taxa de aleatoriedade para atitudes ganacioas do algorítimo.\n",
        "    self.epsilon_final = 0.01 # Taxa final reduzida\n",
        "    self.epsilon_decay = 0.995 # Velocidade de decaimento da taxa\n",
        "\n",
        "    self.model = self.model_builder() # Inicializa um modelo e de rede neural e salva na classe\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # DEFININDO A REDE NEURAL\n",
        "\n",
        "  def model_builder(self):\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(layers.Dense(units=32, activation='relu', input_dim=self.state_size))\n",
        "    model.add(layers.Dense(units=64, activation='relu'))\n",
        "    model.add(layers.Dense(units=128, activation='relu'))\n",
        "    model.add(layers.Dense(units=self.action_space, activation='linear')) # De maneira geral, teremos 3 saída na rede geral (número de espaços de ação)\n",
        "\n",
        "\n",
        "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001)); # Compilamos o modelo\n",
        "\n",
        "    return model # Retornamos o modelo pela função.\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # FUNÇÃO DE TRADE\n",
        "  # Usa o Epsilon e um número aleatório para definir se usará um dado aleatório ou a previsão da rede.\n",
        "\n",
        "  def trade(self, state):\n",
        "    if random.random() <= self.epsilon:\n",
        "      return random.randrange(self.action_space) # Retonar uma resposta aleatória\n",
        "\n",
        "    actions = self.model.predict(state)\n",
        "    return np.argmax(actions[0]) # Retorna o index da maior resposta da rede\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # LOTE DE TREINAMENTO\n",
        "\n",
        "  # Definindo o modelo para treinamento do lote\n",
        "\n",
        "  def batch_train(self, batch_size): # Função que tem o tamanho do lote como argumento\n",
        "\n",
        "    batch = [] # Iremos usar a memória como lote, por isso iniciamos com uma lista vazia\n",
        "\n",
        "    # Iteramos sobre a memória, adicionando seus elementos ao lote batch\n",
        "    for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n",
        "      batch.append(self.memory[i])\n",
        "\n",
        "    # Agora temos um lote de dados e devemos iterar sobre cada estado, recompensa,\n",
        "    # proximo_estado e conclusão do lote e treinar o modelo com isso.\n",
        "    for state, action, reward, next_state, done in batch:\n",
        "      reward = reward\n",
        "\n",
        "      # Se não estivermos no último agente da memória, então calculamos a\n",
        "      # recompensa descontando a recompensa total da recompensa atual.\n",
        "      if not done:\n",
        "        reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "\n",
        "      # Fazemos uma previsão e alocamos à varivel target\n",
        "      target = self.model.predict(state)\n",
        "      target[0][action] = reward\n",
        "\n",
        "      # Treinamos o modelo com o estado, usando a previsão como resultado esperado.\n",
        "      self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "    # Por fim decrementamos o epsilon a fim de gradativamente diminuir tentativas ganaciosas.\n",
        "    if self.epsilon > self.epsilon_final:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "nMWegySnF4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock Market Data Preprocessing\n",
        "\n",
        "# Definiremos algumas funções auxiliares\n",
        "\n",
        "# Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# Função para formatar texto\n",
        "def stock_price_format(n):\n",
        "  if n < 0:\n",
        "    return \"- # {0:2f}\".format(abs(n))\n",
        "  else:\n",
        "    return \"$ {0:2f}\".format(abs(n))\n",
        "\n",
        "# Busca dados no Yahoo Finance\n",
        "# Formato data = \"yyyy-mm-dd\"\n",
        "def dataset_loader(stock_name, initial_date, final_date):\n",
        "\n",
        "  yfin.pdr_override()\n",
        "\n",
        "  dataset = pdr.get_data_yahoo(stock_name, start=initial_date, end=final_date)\n",
        "\n",
        "  start_date = str(dataset.index[0]).split()[0]\n",
        "  end_date = str(dataset.index[1]).split()[0]\n",
        "\n",
        "  close = dataset['Close']\n",
        "\n",
        "  return close"
      ],
      "metadata": {
        "id": "mlwcYt-3fItG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State Creator\n",
        "\n",
        "\n",
        "def state_creator(data, timestep, window_size):\n",
        "\n",
        "  # O index incial (starting_id) será o timestep (passos/dias que já foram dados)\n",
        "  # menos o tamanho da janela, que serão os dias olhados para trás.\n",
        "  starting_id = timestep - window_size + 1\n",
        "\n",
        "\n",
        "  # Lógica para preencher os dados vindos da tabela Data, no array windowed_data\n",
        "\n",
        "  if starting_id >= 0: # No geral este será a condição sempre executada\n",
        "    windowed_data = data[starting_id: timestep + 1]\n",
        "\n",
        "  else: # Condição executada apenas nos primeiros valores\n",
        "    windowed_data =- starting_id * [data[0]] + list(data[0:timestep + 1])\n",
        "\n",
        "  state = [] # Criação de uma array para retorno, com o estado.\n",
        "\n",
        "  for i in range(window_size - 1):\n",
        "    state.append(sigmoid(windowed_data[i + 1] - windowed_data[i]))\n",
        "\n",
        "  return np.array([state])"
      ],
      "metadata": {
        "id": "v6ZXzg8GjLM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a Dataset\n",
        "\n",
        "# CONFIGURAÇÕES DE IMPORTAÇÃO DE DADOS\n",
        "\n",
        "# NOME DA AÇÃO\n",
        "STOCK_NAME = \"VALE3.SA\"\n",
        "\n",
        "# DATA INCIAL\n",
        "INITIAL_DATE = \"2023-01-01\"\n",
        "\n",
        "# DATA FINAL\n",
        "today = datetime.date.today()\n",
        "FINAL_DATE = today.strftime(\"%Y-%m-%d\") # Escolhe a data final como hoje\n",
        "\n",
        "data = dataset_loader(STOCK_NAME, INITIAL_DATE, FINAL_DATE);\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "subYTitYk75X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Q-Learning Trading Agent\n",
        "\n",
        "window_size = 10\n",
        "episodes = 2\n",
        "\n",
        "batch_size = 32\n",
        "data_samples = len(data) - 1\n",
        "\n",
        "trader = AI_Trader(window_size)\n",
        "trader.model.summary()"
      ],
      "metadata": {
        "id": "lNC1nFOYsk20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a Training Loop\n",
        "\n",
        "# Vamos iterar sobre todos episódios\n",
        "\n",
        "for episode in range(1, episodes + 1):\n",
        "\n",
        "  print(\"Episode: {}/{}\".format(episode, episodes))\n",
        "\n",
        "  # Criamos o primeiro estado.\n",
        "  state = state_creator(data, 0, window_size + 1)\n",
        "\n",
        "  # Inicializamos algumas variáveis\n",
        "  total_profit = 0\n",
        "  trader.inventory = []\n",
        "\n",
        "  # O Loop de treinamento que será executado durante uma época inteira\n",
        "  for t in tqdm(range(data_samples)):\n",
        "\n",
        "    # O IA executa a função trade, que responderá com a ação que deve ser tomada\n",
        "    action = trader.trade(state)\n",
        "\n",
        "    # Já definimos o próximo estado\n",
        "    # Note que o definimos com t+1, pois estamos considerando o próximo\n",
        "    # valor da ação no index da tabela de dados.\n",
        "    next_state = state_creator(data, t+1, window_size + 1)\n",
        "\n",
        "    # Sem recompensas até agora\n",
        "    reward = 0\n",
        "\n",
        "    # Sem ação\n",
        "    if action == 0:\n",
        "      # Apenas um print e Recompensa = 0\n",
        "      print(\" - Sem ação | Total de papeis no portfolio = \", len(trader.inventory))\n",
        "\n",
        "    # Compra\n",
        "    if action == 1:\n",
        "      # Recompensa = 0\n",
        "\n",
        "      # Adicionamos a ação comprada na array de portfolio\n",
        "      trader.inventory.append(data[t])\n",
        "\n",
        "      print(\" - AI Trader Comprou: \", stock_price_format(data[t]))\n",
        "\n",
        "    # Venda (Deve possuir ações no portfolio)\n",
        "    elif action == 2 and len(trader.inventory) > 0:\n",
        "\n",
        "      # Remove última ação do portfólio e a retorna\n",
        "      buy_price = trader.inventory.pop(0)\n",
        "\n",
        "      # Recompensa = lucro ou 0 se houve prejuízo.\n",
        "      reward = max(data[t] - buy_price, 0)\n",
        "\n",
        "      total_profit += data[t] - buy_price # Soma ao lucro/prejuízo total\n",
        "\n",
        "      print(\" - AI Trader Vendeu: \", stock_price_format(data[t]), \" - Lucro: \" + stock_price_format(data[t] - buy_price) )\n",
        "\n",
        "\n",
        "    # Verifica se estamos no final de uma época\n",
        "    if t == data_samples - 1:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "\n",
        "\n",
        "    # Salvamos os dados na memória, na mesma ordem que na função BATCH_TRAIN\n",
        "    trader.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # Definimos que o estado atual é o próximo estado calculado anteriormente\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "      print(\"########################\")\n",
        "      print(\"TOTAL PROFIT: {}\".format(total_profit))\n",
        "      print(\"########################\")\n",
        "\n",
        "\n",
        "    # Se o tamanho da memória for maior que o tamanho do lote que definimos\n",
        "    # Então vamos treinar a rede, passando o tamanho do lote como argumento\n",
        "    if len(trader.memory) > batch_size:\n",
        "      trader.batch_train(batch_size)\n",
        "\n",
        "  # A Cada 10 episódios treinados, salvamos a rede\n",
        "  if episode % 10 == 0:\n",
        "    trader.model.save(\"ai_trader_{}.h5\".format(episode))\n"
      ],
      "metadata": {
        "id": "_nZd_fkru7y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}