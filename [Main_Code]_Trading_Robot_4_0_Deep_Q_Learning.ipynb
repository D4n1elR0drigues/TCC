{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D4n1elR0drigues/TCC/blob/main/%5BMain_Code%5D_Trading_Robot_4_0_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTAR NA PRIMEIRA EXECUÇÃO!"
      ],
      "metadata": {
        "id": "ejU_s_D7j6b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader as data_reader\n",
        "\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "from collections import deque\n",
        "\n",
        "import pandas\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yfin\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "\n",
        "# Monte o Google Drive no ambiente do Colab\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "QTd21an3F2a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044b8d35-1a73-4eb4-ff3d-58e8572143dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our Deep Q-Learning Trader\n",
        "\n",
        "class AI_Trader():\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # CONSTRUTOR\n",
        "\n",
        "  def __init__(self, state_size, action_space=3, model_name=\"AITrader\"):\n",
        "\n",
        "    self.state_size = state_size # Tamanho da entrada da rede neural\n",
        "    self.action_space = action_space # Espaço de ação será 3, Comprar, Vender, Sem Ação (Tamanho da saída da rede neural)\n",
        "    self.memory = deque(maxlen=2000) # Memória com 2000 posições. A função Deque permite adicionar elementos ao final, enquanto remove elementos do início.\n",
        "    self.inventory = [] # Terá as comprar que já fizemos\n",
        "    self.model_name = model_name # Nome do modelo para o Keras\n",
        "\n",
        "    self.gamma = 0.95 # Parâmetro que ajudará a maximizar a recompensa\n",
        "    self.epsilon = 1.0 # Taxa de aleatoriedade para atitudes ganacioas do algorítimo.\n",
        "    self.epsilon_final = 0.01 # Taxa final reduzida\n",
        "    self.epsilon_decay = 0.995 # Velocidade de decaimento da taxa\n",
        "\n",
        "    self.model = self.model_builder() # Inicializa um modelo e de rede neural e salva na classe\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # DEFININDO A REDE NEURAL\n",
        "\n",
        "  def model_builder(self):\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(layers.Dense(units=32, activation='relu', input_dim=self.state_size))\n",
        "    model.add(layers.Dense(units=64, activation='relu'))\n",
        "    model.add(layers.Dense(units=128, activation='relu'))\n",
        "    model.add(layers.Dense(units=self.action_space, activation='linear')) # De maneira geral, teremos 3 saída na rede geral (número de espaços de ação)\n",
        "\n",
        "\n",
        "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001)); # Compilamos o modelo\n",
        "\n",
        "    return model # Retornamos o modelo pela função.\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # FUNÇÃO DE TRADE\n",
        "  # Usa o Epsilon e um número aleatório para definir se usará um dado aleatório ou a previsão da rede.\n",
        "\n",
        "  def trade(self, state):\n",
        "    if random.random() <= self.epsilon:\n",
        "      return random.randrange(self.action_space) # Retonar uma resposta aleatória\n",
        "\n",
        "    actions = self.model.predict(state)\n",
        "    return np.argmax(actions[0]) # Retorna o index da maior resposta da rede\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "  # LOTE DE TREINAMENTO\n",
        "\n",
        "  # Definindo o modelo para treinamento do lote\n",
        "\n",
        "  def batch_train(self, batch_size): # Função que tem o tamanho do lote como argumento\n",
        "\n",
        "    batch = [] # Iremos usar a memória como lote, por isso iniciamos com uma lista vazia\n",
        "\n",
        "    # Iteramos sobre a memória, adicionando seus elementos ao lote batch\n",
        "    for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n",
        "      batch.append(self.memory[i])\n",
        "\n",
        "    # Agora temos um lote de dados e devemos iterar sobre cada estado, recompensa,\n",
        "    # proximo_estado e conclusão do lote e treinar o modelo com isso.\n",
        "    for state, action, reward, next_state, done in batch:\n",
        "      reward = reward\n",
        "\n",
        "      # Se não estivermos no último agente da memória, então calculamos a\n",
        "      # recompensa descontando a recompensa total da recompensa atual.\n",
        "      if not done:\n",
        "        reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "\n",
        "      # Fazemos uma previsão e alocamos à varivel target\n",
        "      target = self.model.predict(state)\n",
        "      target[0][action] = reward\n",
        "\n",
        "      # Treinamos o modelo com o estado, usando a previsão como resultado esperado.\n",
        "      self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "    # Por fim decrementamos o epsilon a fim de gradativamente diminuir tentativas ganaciosas.\n",
        "    if self.epsilon > self.epsilon_final:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "nMWegySnF4jR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock Market Data Preprocessing\n",
        "\n",
        "# Definiremos algumas funções auxiliares\n",
        "\n",
        "# Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# Função para formatar texto\n",
        "def stock_price_format(n):\n",
        "  if n < 0:\n",
        "    return \"- # {0:2f}\".format(abs(n))\n",
        "  else:\n",
        "    return \"$ {0:2f}\".format(abs(n))\n",
        "\n",
        "# Busca dados no Yahoo Finance\n",
        "# Formato data = \"yyyy-mm-dd\"\n",
        "def dataset_loader(stock_name, initial_date, final_date):\n",
        "\n",
        "  yfin.pdr_override()\n",
        "\n",
        "  dataset = pdr.get_data_yahoo(stock_name, start=initial_date, end=final_date)\n",
        "\n",
        "  start_date = str(dataset.index[0]).split()[0]\n",
        "  end_date = str(dataset.index[1]).split()[0]\n",
        "\n",
        "  close = dataset['Close']\n",
        "\n",
        "  return close"
      ],
      "metadata": {
        "id": "mlwcYt-3fItG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# State Creator\n",
        "\n",
        "\n",
        "def state_creator(data, timestep, window_size):\n",
        "\n",
        "  # O index incial (starting_id) será o timestep (passos/dias que já foram dados)\n",
        "  # menos o tamanho da janela, que serão os dias olhados para trás.\n",
        "  starting_id = timestep - window_size + 1\n",
        "\n",
        "\n",
        "  # Lógica para preencher os dados vindos da tabela Data, no array windowed_data\n",
        "\n",
        "  if starting_id >= 0: # No geral este será a condição sempre executada\n",
        "    windowed_data = data[starting_id: timestep + 1]\n",
        "\n",
        "  else: # Condição executada apenas nos primeiros valores\n",
        "    windowed_data =- starting_id * [data[0]] + list(data[0:timestep + 1])\n",
        "\n",
        "  state = [] # Criação de uma array para retorno, com o estado.\n",
        "\n",
        "  for i in range(window_size - 1):\n",
        "    state.append(sigmoid(windowed_data[i + 1] - windowed_data[i]))\n",
        "\n",
        "  return np.array([state])"
      ],
      "metadata": {
        "id": "v6ZXzg8GjLM1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a Dataset\n",
        "\n",
        "# CONFIGURAÇÕES DE IMPORTAÇÃO DE DADOS\n",
        "\n",
        "# NOME DA AÇÃO\n",
        "STOCK_NAME = \"VALE3.SA\"\n",
        "\n",
        "# DATA INCIAL\n",
        "INITIAL_DATE = \"2023-05-01\"\n",
        "\n",
        "# DATA FINAL\n",
        "today = datetime.date.today()\n",
        "FINAL_DATE = today.strftime(\"%Y-%m-%d\") # Escolhe a data final como hoje\n",
        "\n",
        "data = dataset_loader(STOCK_NAME, INITIAL_DATE, FINAL_DATE);\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "subYTitYk75X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfaec5c7-e684-4aad-c07c-61bcd8ba3e92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2023-05-02    69.540001\n",
              "2023-05-03    69.000000\n",
              "2023-05-04    66.860001\n",
              "2023-05-05    69.040001\n",
              "2023-05-08    69.970001\n",
              "                ...    \n",
              "2023-10-18    65.360001\n",
              "2023-10-19    64.419998\n",
              "2023-10-20    62.680000\n",
              "2023-10-23    62.560001\n",
              "2023-10-24    63.990002\n",
              "Name: Close, Length: 123, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Q-Learning Trading Agent\n",
        "\n",
        "window_size = 10\n",
        "episodes = 2\n",
        "\n",
        "batch_size = 32\n",
        "data_samples = len(data) - 1\n",
        "\n",
        "trader = AI_Trader(window_size)\n",
        "trader.model.summary()"
      ],
      "metadata": {
        "id": "lNC1nFOYsk20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76209de3-b545-418e-94e6-f3b3154848db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                352       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11171 (43.64 KB)\n",
            "Trainable params: 11171 (43.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos iterar sobre todos episódios\n",
        "\n",
        "for episode in range(1, episodes + 1):\n",
        "\n",
        "  print(\"Episode: {}/{}\".format(episode, episodes))\n",
        "\n",
        "  # Criamos o primeiro estado.\n",
        "  state = state_creator(data, 0, window_size + 1)\n",
        "\n",
        "  # Inicializamos algumas variáveis\n",
        "  total_profit = 0\n",
        "  trader.inventory = []\n",
        "\n",
        "  # O Loop de treinamento que será executado durante uma época inteira\n",
        "  for t in tqdm(range(data_samples)):\n",
        "\n",
        "    # O IA executa a função trade, que responderá com a ação que deve ser tomada\n",
        "    action = trader.trade(state)\n",
        "\n",
        "    # Já definimos o próximo estado\n",
        "    # Note que o definimos com t+1, pois estamos considerando o próximo\n",
        "    # valor da ação no index da tabela de dados.\n",
        "    next_state = state_creator(data, t+1, window_size + 1)\n",
        "\n",
        "    # Sem recompensas até agora\n",
        "    reward = 0\n",
        "\n",
        "    # Sem ação\n",
        "    if action == 0:\n",
        "      # Apenas um print e Recompensa = 0\n",
        "      print(\" - Sem ação | Total de papeis no portfolio = \", len(trader.inventory))\n",
        "\n",
        "    # Compra\n",
        "    if action == 1:\n",
        "      # Recompensa = 0\n",
        "\n",
        "      # Adicionamos a ação comprada na array de portfolio\n",
        "      trader.inventory.append(data[t])\n",
        "\n",
        "      print(\" - AI Trader Comprou: \", stock_price_format(data[t]))\n",
        "\n",
        "    # Venda (Deve possuir ações no portfolio)\n",
        "    elif action == 2 and len(trader.inventory) > 0:\n",
        "\n",
        "      # Remove última ação do portfólio e a retorna\n",
        "      buy_price = trader.inventory.pop(0)\n",
        "\n",
        "      # Recompensa = lucro ou 0 se houve prejuízo.\n",
        "      reward = max(data[t] - buy_price, 0)\n",
        "\n",
        "      total_profit += data[t] - buy_price # Soma ao lucro/prejuízo total\n",
        "\n",
        "      print(\" - AI Trader Vendeu: \", stock_price_format(data[t]), \" - Lucro: \" + stock_price_format(data[t] - buy_price) )\n",
        "\n",
        "\n",
        "    # Verifica se estamos no final de uma época\n",
        "    if t == data_samples - 1:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "\n",
        "\n",
        "    # Salvamos os dados na memória, na mesma ordem que na função BATCH_TRAIN\n",
        "    trader.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # Definimos que o estado atual é o próximo estado calculado anteriormente\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "      print(\"########################\")\n",
        "      print(\"TOTAL PROFIT: {}\".format(total_profit))\n",
        "      print(\"########################\")\n",
        "\n",
        "\n",
        "    # Se o tamanho da memória for maior que o tamanho do lote que definimos\n",
        "    # Então vamos treinar a rede, passando o tamanho do lote como argumento\n",
        "    if len(trader.memory) > batch_size:\n",
        "      trader.batch_train(batch_size)\n",
        "\n",
        "  # A Cada 10 episódios treinados, salvamos a rede\n",
        "  if episode % 2 == 0:\n",
        "      model_name = \"ai_trader_{}.h5\".format(episode)\n",
        "      trader.model.save(model_name)\n",
        "\n",
        "      # Enviar o arquivo para o Google Drive\n",
        "      source_path = model_name\n",
        "      destination_path = '/content/drive/My Drive/Arquivos H5/' + model_name\n",
        "\n",
        "      shutil.copy(source_path, destination_path)\n",
        "\n",
        "      print('Arquivo enviado para o Google Drive com sucesso!')"
      ],
      "metadata": {
        "id": "_nZd_fkru7y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina uma função para carregar e executar o modelo treinado\n",
        "def run_trained_model(model_path, data, window_size):\n",
        "    trader = AI_Trader(window_size)\n",
        "    trader.model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    state = state_creator(data, 0, window_size + 1)\n",
        "    total_profit = 0\n",
        "    trader.inventory.clear()\n",
        "    trader.inventory = []\n",
        "    print(\"quantidade 555 \", len(trader.inventory))\n",
        "\n",
        "    for t in tqdm(range(len(data) - 1)):\n",
        "        action = trader.trade(state)\n",
        "        next_state = state_creator(data, t + 1, window_size + 1)\n",
        "\n",
        "        # Sem ação\n",
        "        if action == 0:\n",
        "          # Apenas um print e Recompensa = 0\n",
        "          print(\" - Sem ação | Total de papeis no portfolio = \", len(trader.inventory))\n",
        "\n",
        "        # Compra\n",
        "        if action == 1:\n",
        "          # Recompensa = 0\n",
        "\n",
        "          # Adicionamos a ação comprada na array de portfolio\n",
        "          trader.inventory.append(data[t])\n",
        "\n",
        "          print(\" - AI Trader Comprou: \", stock_price_format(data[t]))\n",
        "\n",
        "        # Venda (Deve possuir ações no portfolio)\n",
        "        elif action == 2 and len(trader.inventory) > 0:\n",
        "\n",
        "          # Remove última ação do portfólio e a retorna\n",
        "          buy_price = trader.inventory.pop(0)\n",
        "\n",
        "          # Recompensa = lucro ou 0 se houve prejuízo.\n",
        "          reward = max(data[t] - buy_price, 0)\n",
        "\n",
        "          total_profit += data[t] - buy_price # Soma ao lucro/prejuízo total\n",
        "\n",
        "          print(\" - AI Trader Vendeu: \", stock_price_format(data[t]), \" - Lucro: \" + stock_price_format(data[t] - buy_price) )\n",
        "        state = next_state\n",
        "\n",
        "    return total_profit\n",
        "\n",
        "# Caminho do modelo no seu Google Drive\n",
        "model_path = \"/content/drive/My Drive/Arquivos H5/ai_trader_2.h5\"  # Substitua pelo caminho correto\n",
        "\n",
        "# Exemplo de uso\n",
        "new_data = dataset_loader(STOCK_NAME, \"2023-01-01\", \"2023-02-01\")  # Carrega novos dados\n",
        "\n",
        "# Executa o modelo treinado nos novos dados\n",
        "profit = run_trained_model(model_path, new_data, window_size)\n",
        "\n",
        "print(f\"Lucro total: {profit}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pLJDQkSbkKR",
        "outputId": "766a0df2-062f-49fe-9aca-395aae4989d3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "quantidade 555  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:00<00:00, 1962.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Sem ação | Total de papeis no portfolio =  0\n",
            " - AI Trader Comprou:  $ 89.239998\n",
            " - AI Trader Comprou:  $ 89.400002\n",
            " - AI Trader Vendeu:  $ 90.900002  - Lucro: $ 1.660004\n",
            " - Sem ação | Total de papeis no portfolio =  1\n",
            " - AI Trader Comprou:  $ 92.440002\n",
            " - AI Trader Comprou:  $ 93.580002\n",
            " - AI Trader Vendeu:  $ 93.250000  - Lucro: $ 3.849998\n",
            " - Sem ação | Total de papeis no portfolio =  2\n",
            " - AI Trader Comprou:  $ 93.820000\n",
            " - AI Trader Comprou:  $ 92.250000\n",
            " - Sem ação | Total de papeis no portfolio =  4\n",
            " - Sem ação | Total de papeis no portfolio =  4\n",
            " - Sem ação | Total de papeis no portfolio =  4\n",
            " - AI Trader Vendeu:  $ 93.989998  - Lucro: $ 1.549995\n",
            " - AI Trader Vendeu:  $ 94.300003  - Lucro: $ 0.720001\n",
            " - AI Trader Vendeu:  $ 95.300003  - Lucro: $ 1.480003\n",
            " - Sem ação | Total de papeis no portfolio =  1\n",
            " - AI Trader Comprou:  $ 98.000000\n",
            " - AI Trader Comprou:  $ 95.320000\n",
            " - AI Trader Vendeu:  $ 94.980003  - Lucro: $ 2.730003\n",
            "Lucro total: 11.990005493164062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}